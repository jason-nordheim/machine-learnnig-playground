# Linear Algebra

Defined, **linear algebra** is the study of vectors and linear functions.

Building Blocks:

- Systems of linear equations
- Vectors & matrices
- Linear transformations
- Determinants
- Vector spaces

Use Cases:

- Predicting numerical values in simplier regression problems

> Most commonly, Linear regression uses a "least squares optimization"

## Applications

- Machine Learning
  - Machine learning requires dealing with large data sets involving multiple rows and columns
  - This data is often represented as vectors or matrices

## Dimensional Reduction

Core Concepts:

- Datasets are represented as matrices
- Dimensional reduction leverages factorization methods to reduce it to its constituent parts

## One Hot Encoding

- Used when working with categorical data
- For example:
  - classification problems
  - categorical input classification

## Regularization

A concept from Linear Algebra that is used to prevent a model from "overfitting"

- often used to encourage a model to minimize the size of coefficients while it is being fit on the data

## Principal Component Analysis

Method for automatically reducing the number of columns in a dataset.

- Used to model data with many features.
- Used in ML to create projections of high dimensional data for both visualization and training models

> Principal Component Analysis is often abbreviated as "PCA"

## Latent Semantic Analysis (LSA)

Form of data preparation used in in natural language processing (NLP), a subfield of machine learning that works with large amounts of text data

## Recommender Systems

Predictive modeling systems that involve the recommendation of products

## Deep Learning

- Components:
  - Scaled up to multiple dimensions
  - Deep learning methods leverage vectors, matrices and tensors of inputs and coefficients

# Vectors in Linear algebra

In Linear Algebra, we assume vectors are **column oriented** and denote these vectors as `v`. If the vector is row oriented, the vector is denoted as `v^t`
